{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ff706c-16ab-46f2-ab44-b9fd9c86c878",
   "metadata": {},
   "source": [
    "My Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53683df8-9490-402a-950e-c0d1d4b5b13e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from  numpy import ndarray\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5399eab6-48f1-4675-b00f-6972129998e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assert_same_shape(array: ndarray,\n",
    "                      array_grad: ndarray):\n",
    "    assert array.shape == array_grad.shape, \\\n",
    "        '''\n",
    "        Two ndarrays should have the same shape;\n",
    "        instead, first ndarray's shape is {0}\n",
    "        and second ndarray's shape is {1}.\n",
    "        '''.format(tuple(array_grad.shape), tuple(array.shape))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7180b5e9-1a14-4e22-ba74-365d19ea5902",
   "metadata": {},
   "source": [
    "Esta será la clase base para toda operación en la red neuronal, como puede ser un producto punto por ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aee8df7-51a7-427f-8dde-2172342b97a2",
   "metadata": {},
   "source": [
    "- input gradient -> será la derivada parcial de cada elemento de la salida "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88df1a45-f8ba-40d3-97d8-344c7800b708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "    \"\"\"\n",
    "    Clase base para toda operación interna en la red neuronal\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, input_:ndarray):\n",
    "        \"\"\"\n",
    "        Almacenamos la entrada y retornamos la salida global de la operación llamando a self.output_()\n",
    "        \"\"\"\n",
    "        self.input_ = input_\n",
    "\n",
    "        self.output = self._output()\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    \n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Ejecuta la función self._input_grad(). Comprueba los tamaños de las matrices\n",
    "        '''\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        return self.input_grad\n",
    " \n",
    "    \n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        El método _output es definido para cada subclase de Operation\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        El método _input_grad es definido para cada subclase de Operation\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b8d3b2-8123-4f75-9144-aaed52281405",
   "metadata": {},
   "source": [
    "Para cada operación, necesitamos realizar un procedimiento específico dependiendo de los parámetros que reciba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36c1de28-0a6f-473d-b1fa-ab51e5e53d87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ParamOperation(Operation):\n",
    "    '''\n",
    "    Realiza una operación (heredando Operation) tomando unos parámetros de entrada\n",
    "    '''\n",
    "\n",
    "    def __init__(self, param: ndarray) -> ndarray:\n",
    "        '''\n",
    "        El método ParamOperation\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Ejecuta self._input_grad y self._param_grad.\n",
    "        Comprueba el tamaño correcto de las matrices.\n",
    "        '''\n",
    "\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        assert_same_shape(self.param, self.param_grad)\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Cada subclase de ParamOperation debe implementar un _param_grad específico.\n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83443bc-7767-4b1b-a774-b0428d81ad6b",
   "metadata": {},
   "source": [
    "Ahora ponemos en marcha operaciones específicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e78a33b-527c-4048-8c8f-fb70a0bbaa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightMultiply(ParamOperation):\n",
    "    '''\n",
    "    Weight multiplication operation for a neural network.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, W: ndarray):\n",
    "        '''\n",
    "        Inincializa la clase Operation con self.param = W.\n",
    "        '''\n",
    "        super().__init__(W)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        Calcula la salida de la operación.\n",
    "        '''\n",
    "        return np.dot(self.input_, self.param)\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Calcula el \"gradiente de entrada\", aplicando la regla de la cadena multiplicando el gradiente anterior por el de este\n",
    "        '''\n",
    "        return np.dot(output_grad, np.transpose(self.param, (1, 0)))\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray)  -> ndarray:\n",
    "        '''\n",
    "        Calcula el gradiente de la operación (X*W), en términos de la entrada X\n",
    "        '''        \n",
    "        return np.dot(np.transpose(self.input_, (1, 0)), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1091f704-ca09-46c2-87cb-ebdf69e4fab6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BiasAdd(ParamOperation):\n",
    "    '''\n",
    "    Calcula la suma del bias.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 B: ndarray):\n",
    "        '''\n",
    "        Inincializa el objeto Operation con self.param = B.\n",
    "        Comprueba el tamaño del vector.\n",
    "        '''\n",
    "        assert B.shape[0] == 1\n",
    "        \n",
    "        super().__init__(B)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        Calcula la operación (entrada + bias)\n",
    "        '''\n",
    "        return self.input_ + self.param\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Calcula el \"gradiente de entrada\", derivando la operación en términos del bias y multiplicando por el gradiente anterior\n",
    "        '''\n",
    "        return np.ones_like(self.input_) * output_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Calcula el gradiente en términos del bias, sumando para cada fila\n",
    "        '''\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf82adfd-b338-45cb-ae70-198d7617ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Operation):\n",
    "    '''\n",
    "    Función de activación Sigmoide\n",
    "    '''\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        '''Pass'''\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        Aplica la función no-lineal\n",
    "        '''\n",
    "        return 1.0/(1.0+np.exp(-1.0 * self.input_))\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Calcula el gradiente, aplicando la regla de la cadena, multiplicando así dicho gradiente por el gradiente anterior\n",
    "        '''\n",
    "        sigmoid_backward = self.output * (1.0 - self.output)\n",
    "        input_grad = sigmoid_backward * output_grad\n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41846c94-8a41-4269-b59a-947ff61c4f36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Linear(Operation):\n",
    "    '''\n",
    "    \"Identity\" activation function\n",
    "    '''\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        '''Pass'''        \n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''Pass through'''\n",
    "        return self.input_\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''Pass through'''\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e4111-284e-4a4e-a386-60f3c14b6b55",
   "metadata": {},
   "source": [
    "Implementando la clase Layer, que será la capa que realice las operaciones pertinentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e7aa062-33cf-484f-a420-3375773b474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    '''\n",
    "    Capa de nodos\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 neurons: int):\n",
    "        '''\n",
    "        El número de nodos corresponde al \"ancho\" de la capa\n",
    "        '''\n",
    "        self.neurons = neurons\n",
    "        self.first = True\n",
    "        self.params: List[ndarray] = []\n",
    "        self.param_grads: List[ndarray] = []\n",
    "        self.operations: List[Operation] = []\n",
    "\n",
    "    def _setup_layer(self, num_in: int) -> None:\n",
    "        '''\n",
    "        La función _setup_layer debe ser implementada por cada instancia de capa\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, input_: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Passes input forward through a series of operations\n",
    "        ''' \n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "\n",
    "        self.input_ = input_\n",
    "\n",
    "        for operation in self.operations:\n",
    "\n",
    "            input_ = operation.forward(input_)\n",
    "\n",
    "        self.output = input_\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Passes output_grad backward through a series of operations\n",
    "        Checks appropriate shapes\n",
    "        '''\n",
    "\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "\n",
    "        input_grad = output_grad\n",
    "        \n",
    "        self._param_grads()\n",
    "\n",
    "        return input_grad\n",
    "\n",
    "    def _param_grads(self) -> ndarray:\n",
    "        '''\n",
    "        Extrae los _param_grads (gradientes) de las operaciones de la capa\n",
    "        '''\n",
    "\n",
    "        self.param_grads = [] #extraemos los gradientes de cada operación dentro de la capa\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "\n",
    "    def _params(self) -> ndarray:\n",
    "        '''\n",
    "        Extrae los valores de los parámetros _params de las operaciones de la capa\n",
    "        '''\n",
    "\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.params.append(operation.param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56302cb-477f-47e8-98ce-9ac470cb8931",
   "metadata": {},
   "source": [
    "Dense heredará el objeto Layer (refiriéndose a la fully-connected-layer) donde serán especificados los parámetros de la capa actual, como son el nº de nodos y la función no-lineal de activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18be0232-89d3-4fab-8c19-00eccfffcb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    '''\n",
    "    Una capa totalmente conectada heredada de \"Layer\"\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 neurons: int,\n",
    "                 activation: Operation = Sigmoid()):\n",
    "        '''\n",
    "        Requiere una función no-lineal de activación\n",
    "        '''\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "\n",
    "    def _setup_layer(self, input_: ndarray) -> None:\n",
    "        '''\n",
    "        Definimos las operaciones en la capa\n",
    "        '''\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        self.params = []\n",
    "\n",
    "        # weights\n",
    "        self.params.append(np.random.randn(input_.shape[1], self.neurons))\n",
    "\n",
    "        # bias\n",
    "        self.params.append(np.random.randn(1, self.neurons))\n",
    "\n",
    "        self.operations = [WeightMultiply(self.params[0]), #básicamente, será la multiplicación de entrada por pesos, adicción de bias\n",
    "                           BiasAdd(self.params[1]), \n",
    "                           self.activation] #y por último, aplicar la activación\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca76b5c9-fe63-4b68-85e7-148ed45c75d3",
   "metadata": {},
   "source": [
    "La clase pérdida tomará los gradientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae4c3edc-4103-4f4b-82ac-fef03a6190f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    '''\n",
    "    La \"pérdida\" de la NN\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''Pass'''\n",
    "        pass\n",
    "\n",
    "    def forward(self, prediction: ndarray, target: ndarray) -> float:\n",
    "        '''\n",
    "        Calcula el valor actual de la pérdida\n",
    "        '''\n",
    "        assert_same_shape(prediction, target)\n",
    "\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "\n",
    "        loss_value = self._output()\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def backward(self) -> ndarray:\n",
    "        '''\n",
    "        Computes gradient of the loss value with respect to the input to the loss function\n",
    "        '''\n",
    "        self.input_grad = self._input_grad()\n",
    "\n",
    "        assert_same_shape(self.prediction, self.input_grad)\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        '''\n",
    "        Cada subclase de \"Loss\" debe implementar la función _output.\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        '''\n",
    "        Cada subclase de \"Loss\" debe implementar la función _input_grad.\n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5b29e71-d3f0-4ec6-acec-6af8a96eb836",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeanSquaredError(Loss):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        '''Pass'''\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        '''\n",
    "        Calcula la pérdida MSE de las predicciones\n",
    "        '''\n",
    "        loss = (\n",
    "            np.sum(np.power(self.prediction - self.target, 2)) / \n",
    "            self.prediction.shape[0]\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        '''\n",
    "        Calcula el gradiente de la salida de la función de pérdidas MSE en términos de la predicción\n",
    "        '''        \n",
    "\n",
    "        return 2.0 * (self.prediction - self.target) / self.prediction.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6d7579-9c4c-4dc0-8583-1db2296cf94e",
   "metadata": {},
   "source": [
    "Finalmente, la clase de más alto nivel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d50f989-631b-4f48-ad22-a7c81663ba16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    '''\n",
    "    La clase referente a la NN\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 layers: List[Layer],\n",
    "                 loss: Loss,\n",
    "                 seed: int = 1) -> None:\n",
    "        '''\n",
    "        La NN necesita una lista de las capas, y un objeto de la función de pérdidas.\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.seed = seed\n",
    "        if seed:\n",
    "            for layer in self.layers:\n",
    "                setattr(layer, \"seed\", self.seed)        \n",
    "\n",
    "    def forward(self, x_batch: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Pasa la entrada a través de la serie de capas\n",
    "        '''\n",
    "        x_out = x_batch\n",
    "        for layer in self.layers:\n",
    "            x_out = layer.forward(x_out)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "    def backward(self, loss_grad: ndarray) -> None:\n",
    "        '''\n",
    "        Recoge los gradientes hacia atrás de las capas\n",
    "        '''\n",
    "\n",
    "        grad = loss_grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def train_batch(self,\n",
    "                    x_batch: ndarray,\n",
    "                    y_batch: ndarray) -> float:\n",
    "        '''\n",
    "        - Pasa los datos hacia delante a través de las capas\n",
    "        - Calcula el valor de pérdida\n",
    "        - Pasa los gradientes hacia atrás para actualizar los pesos\n",
    "        '''\n",
    "        \n",
    "        predictions = self.forward(x_batch)\n",
    "\n",
    "        loss = self.loss.forward(predictions, y_batch)\n",
    "\n",
    "        self.backward(self.loss.backward())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def params(self):\n",
    "        '''\n",
    "        Obtiene los parámetros de la red\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            yield from layer.params\n",
    "\n",
    "    def param_grads(self):\n",
    "        '''\n",
    "        Obtiene el gradiente de la pérdida en términos de los parámetros de cada capa de la red\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            yield from layer.param_grads    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ecbefc-c74f-49ba-8337-2a2c88030d99",
   "metadata": {},
   "source": [
    "Optimizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c112253-cbe5-469c-be92-8c4adda32c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    '''\n",
    "    Base class for a neural network optimizer.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01):\n",
    "        '''\n",
    "        Every optimizer must have an initial learning rate.\n",
    "        '''\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self) -> None:\n",
    "        '''\n",
    "        Every optimizer must implement the \"step\" function.\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39de98fe-fa8e-4d8f-a9fc-1cc84200d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    '''\n",
    "    Stochasitc gradient descent optimizer.\n",
    "    '''    \n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01) -> None:\n",
    "        '''Pass'''\n",
    "        super().__init__(lr)\n",
    "\n",
    "    def step(self):\n",
    "        '''\n",
    "        For each parameter, adjust in the appropriate direction, with the magnitude of the adjustment \n",
    "        based on the learning rate.\n",
    "        '''\n",
    "        for (param, param_grad) in zip(self.net.params(),\n",
    "                                       self.net.param_grads()):\n",
    "\n",
    "            param -= self.lr * param_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528b8a52-c1d3-45dc-b343-271302ea8fb1",
   "metadata": {},
   "source": [
    "# `Trainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b95c6e3c-b59d-4c6b-87f9-31b503492a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "\n",
    "class Trainer(object):\n",
    "    '''\n",
    "    Trains a neural network\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 net: NeuralNetwork,\n",
    "                 optim: Optimizer) -> None:\n",
    "        '''\n",
    "        Requires a neural network and an optimizer in order for training to occur. \n",
    "        Assign the neural network as an instance variable to the optimizer.\n",
    "        '''\n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.best_loss = 1e9\n",
    "        setattr(self.optim, 'net', self.net)\n",
    "        \n",
    "    def generate_batches(self,\n",
    "                         X: ndarray,\n",
    "                         y: ndarray,\n",
    "                         size: int = 32) -> Tuple[ndarray]:\n",
    "        '''\n",
    "        Generates batches for training \n",
    "        '''\n",
    "        assert X.shape[0] == y.shape[0], \\\n",
    "        '''\n",
    "        features and target must have the same number of rows, instead\n",
    "        features has {0} and target has {1}\n",
    "        '''.format(X.shape[0], y.shape[0])\n",
    "\n",
    "        N = X.shape[0]\n",
    "\n",
    "        for ii in range(0, N, size):\n",
    "            X_batch, y_batch = X[ii:ii+size], y[ii:ii+size]\n",
    "\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "            \n",
    "    def fit(self, X_train: ndarray, y_train: ndarray,\n",
    "            X_test: ndarray, y_test: ndarray,\n",
    "            epochs: int=100,\n",
    "            eval_every: int=10,\n",
    "            batch_size: int=32,\n",
    "            seed: int = 1,\n",
    "            restart: bool = True)-> None:\n",
    "        '''\n",
    "        Fits the neural network on the training data for a certain number of epochs.\n",
    "        Every \"eval_every\" epochs, it evaluated the neural network on the testing data.\n",
    "        '''\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        if restart:\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "\n",
    "            self.best_loss = 1e9\n",
    "\n",
    "        for e in range(epochs):\n",
    "\n",
    "            if (e+1) % eval_every == 0:\n",
    "                \n",
    "                # for early stopping\n",
    "                last_model = deepcopy(self.net)\n",
    "\n",
    "            X_train, y_train = permute_data(X_train, y_train)\n",
    "\n",
    "            batch_generator = self.generate_batches(X_train, y_train,\n",
    "                                                    batch_size)\n",
    "\n",
    "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "\n",
    "                self.net.train_batch(X_batch, y_batch)\n",
    "\n",
    "                self.optim.step()\n",
    "\n",
    "            if (e+1) % eval_every == 0:\n",
    "\n",
    "                test_preds = self.net.forward(X_test)\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "\n",
    "                if loss < self.best_loss:\n",
    "                    print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n",
    "                    self.best_loss = loss\n",
    "                else:\n",
    "                    print(f\"\"\"Loss increased after epoch {e+1}, final loss was {self.best_loss:.3f}, using the model from epoch {e+1-eval_every}\"\"\")\n",
    "                    self.net = last_model\n",
    "                    # ensure self.optim is still updating self.net\n",
    "                    setattr(self.optim, 'net', self.net)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97100b3b-efe1-4dd6-88c4-b88c094eeb68",
   "metadata": {},
   "source": [
    "#### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54948b93-334f-4878-a62e-03d0a1f73133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true: ndarray, y_pred: ndarray):\n",
    "    '''\n",
    "    Compute mean absolute error for a neural network.\n",
    "    '''    \n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def rmse(y_true: ndarray, y_pred: ndarray):\n",
    "    '''\n",
    "    Compute root mean squared error for a neural network.\n",
    "    '''\n",
    "    return np.sqrt(np.mean(np.power(y_true - y_pred, 2)))\n",
    "\n",
    "def eval_regression_model(model: NeuralNetwork,\n",
    "                          X_test: ndarray,\n",
    "                          y_test: ndarray):\n",
    "    '''\n",
    "    Compute mae and rmse for a neural network.\n",
    "    '''\n",
    "    preds = model.forward(X_test)\n",
    "    preds = preds.reshape(-1, 1)\n",
    "    print(\"Mean absolute error: {:.2f}\".format(mae(preds, y_test)))\n",
    "    print()\n",
    "    print(\"Root mean squared error {:.2f}\".format(rmse(preds, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb95ecff-23d9-4006-8d9e-88d2d7c4e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = NeuralNetwork(\n",
    "    layers=[Dense(neurons=1,\n",
    "                   activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")\n",
    "\n",
    "nn = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13,\n",
    "                   activation=Sigmoid()),\n",
    "            Dense(neurons=1,\n",
    "                   activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")\n",
    "\n",
    "dl = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13,\n",
    "                   activation=Sigmoid()),\n",
    "            Dense(neurons=13,\n",
    "                   activation=Sigmoid()),\n",
    "            Dense(neurons=1,\n",
    "                   activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16d389ba-e547-466e-8282-a320c3c00068",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.NeuralNetwork at 0x208f18b4610>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815a95bd-1c27-41fc-927c-388b804a9953",
   "metadata": {},
   "source": [
    "### Read in the data, train-test split etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df640cf3-1189-4945-9c01-a51c43a8e081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ef94631-5c82-43ca-ae55-a36ba32d7624",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_boston\n",
    "boston = pd.read_csv(\"boston.csv\", index_col=[0])\n",
    "\n",
    "data = boston.drop(columns=\"Price\").values\n",
    "target = boston.Price.values\n",
    "features = boston.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3642152b-95d5-4321-9ed8-a3494babacc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15d6d8bd-5f59-4f40-bb66-4676916e1cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_2d_np(a: ndarray, \n",
    "          type: str=\"col\") -> ndarray:\n",
    "    '''\n",
    "    Turns a 1D Tensor into 2D\n",
    "    '''\n",
    "\n",
    "    assert a.ndim == 1, \\\n",
    "    \"Input tensors must be 1 dimensional\"\n",
    "    \n",
    "    if type == \"col\":        \n",
    "        return a.reshape(-1, 1)\n",
    "    elif type == \"row\":\n",
    "        return a.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37f494fd-981b-4ae6-b735-e28ed8aabd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
    "\n",
    "# make target 2d array\n",
    "y_train, y_test = to_2d_np(y_train), to_2d_np(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e94ec3-ea1c-4e48-b94f-02cd12df98b6",
   "metadata": {},
   "source": [
    "### Train the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04d35d3c-e50d-4001-abec-dff7a746587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "\n",
    "def permute_data(X, y):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    return X[perm], y[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf49c646-4236-4801-bc10-275f388873fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 30.293\n",
      "Validation loss after 20 epochs is 28.469\n",
      "Validation loss after 30 epochs is 26.293\n",
      "Validation loss after 40 epochs is 25.541\n",
      "Validation loss after 50 epochs is 25.087\n",
      "\n",
      "Mean absolute error: 3.52\n",
      "\n",
      "Root mean squared error 5.01\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(lr, SGD(lr=0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=20190501);\n",
    "print()\n",
    "eval_regression_model(lr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24d621d4-97dd-4a51-88df-8009640aae8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 27.435\n",
      "Validation loss after 20 epochs is 21.839\n",
      "Validation loss after 30 epochs is 18.918\n",
      "Validation loss after 40 epochs is 17.195\n",
      "Validation loss after 50 epochs is 16.215\n",
      "\n",
      "Mean absolute error: 2.60\n",
      "\n",
      "Root mean squared error 4.03\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(nn, SGD(lr=0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=20190501);\n",
    "print()\n",
    "eval_regression_model(nn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893e326-6468-4583-a046-b5a6ed4b1774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c458f2c7-2f70-40da-837c-b03ed2cdcb2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_workflow]",
   "language": "python",
   "name": "conda-env-pytorch_workflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
