{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ecc2065-3d94-4449-ad6d-cea5f642fff7",
   "metadata": {},
   "source": [
    "## Splitting the data into `train`, `test` & `devset`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cceeee5-e455-44f8-bb51-ef3cb44fe58b",
   "metadata": {},
   "source": [
    "<img src=\"lifecycle.png\" alt=\"architecture info\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd8e4fe-889d-4a01-b762-6dfec12091ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b1deb-f7e8-414e-a652-fb0d2157d1f5",
   "metadata": {},
   "source": [
    "Creamos datos de \"juguete\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7278c-1a81-496a-b8c8-3758083794b2",
   "metadata": {},
   "source": [
    "<img src=\"https://img-b.udemycdn.com/redactor/raw/q_and_a_edit/2023-06-25_19-13-53-632d851a0d6feb00a56a312585e264da.png\n",
    "\" alt=\"architecture info\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb00b7a-b6fc-4c6d-9201-0e42d7e83643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#creamos un dataset falso de \"juguete\"\n",
    "fake_data = np.tile(np.array([1,2,3,4]),(10,1)) +np.tile(10*np.arange(1,11), (4,1)).T\n",
    "#usando np.tile() especificamos un nº de repeticiones concretas (2do argumento) de una matriz dada\n",
    "#y combinamos dos matrices generando un \"dataset\" de 10 observaciones x 4 features\n",
    "\n",
    "fake_labels = np.arange(10)>4 #de un vector del 0 al 10, validamos si son mayores que 4, y asignamos True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96cffed-da23-4687-9117-7f924bf25d51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11,  12,  13,  14],\n",
       "       [ 21,  22,  23,  24],\n",
       "       [ 31,  32,  33,  34],\n",
       "       [ 41,  42,  43,  44],\n",
       "       [ 51,  52,  53,  54],\n",
       "       [ 61,  62,  63,  64],\n",
       "       [ 71,  72,  73,  74],\n",
       "       [ 81,  82,  83,  84],\n",
       "       [ 91,  92,  93,  94],\n",
       "       [101, 102, 103, 104]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421b337c-9b85-4c39-a903-3719fb1e65d2",
   "metadata": {},
   "source": [
    "El objetivo será dividir este set de \"juguete\" en un 80% para training, un 10% para devset y el 10% restante para test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0be458f-daed-4e87-9dcc-4643ccf9c536",
   "metadata": {},
   "source": [
    "### Implementando la división usando `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3cb9283-4e2c-40bf-9a48-af74c1c7d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#especificamos los tamaños (ratios) de las particiones\n",
    "#el orden es train, devset, test\n",
    "partitions = [0.8, 0.1, 0.1]\n",
    "\n",
    "#dividimos los datos (al no poder hacerlo directamente usando train_test_split()),\n",
    "#usaremos esta técnica de doble partición\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(fake_data, #dividiremos un 80% para training y el restante voolverá a ser segmentado\n",
    "                                                                   fake_labels,\n",
    "                                                                   \n",
    "                                                                   train_size=partitions[0])\n",
    "\n",
    "#ahora, dividamos sobre \"X_temp\" e \"y_temp\" para obtener devset y test set\n",
    "temp_split = partitions[1] / np.sum(partitions[1:]) #en este caso, la mitad del set \"temp\" resultante\n",
    "#obtenemos la razón o \"ratio\" que vamos a usar para dividirlo\n",
    "\n",
    "#con train_size nos referimos a la proporción o tamaño del primer dataset (devset)\n",
    "X_devset, X_test, y_devset, y_test = train_test_split(X_temp, y_temp,\n",
    "                                                     train_size=temp_split) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c83987ab-117d-4d1e-8e2c-3e84bbaa10a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: (8, 4)\n",
      "Devset data size: (1, 4)\n",
      "Test data size: (1, 4)\n",
      " \n",
      "Training data: \n",
      "[[ 41  42  43  44]\n",
      " [101 102 103 104]\n",
      " [ 21  22  23  24]\n",
      " [ 91  92  93  94]\n",
      " [ 71  72  73  74]\n",
      " [ 51  52  53  54]\n",
      " [ 61  62  63  64]\n",
      " [ 81  82  83  84]]\n",
      " \n",
      "Devset data: \n",
      "[[11 12 13 14]]\n",
      " \n",
      "Test data: \n",
      "[[31 32 33 34]]\n"
     ]
    }
   ],
   "source": [
    "print('Training data size: ' + str(X_train.shape))\n",
    "print('Devset data size: '   + str(X_devset.shape))\n",
    "print('Test data size: '     + str(X_test.shape))\n",
    "print(' ')\n",
    "\n",
    "# print out the train/test data\n",
    "print('Training data: ')\n",
    "print(X_train)\n",
    "print(' ')\n",
    "\n",
    "print('Devset data: ')\n",
    "print(X_devset)\n",
    "print(' ')\n",
    "\n",
    "print('Test data: ')\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2df25e-b298-4d2e-9b41-03a503ae42fa",
   "metadata": {},
   "source": [
    "## División de forma manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34c6e796-70dc-4f8e-9629-5db8c72976bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition proportions:\n",
      "[0.8 0.1 0.1]\n",
      " \n",
      "Partition boundaries:\n",
      "[ 8  9 10]\n",
      " \n",
      "Randomized data indices:\n",
      "[9 1 4 3 2 5 6 7 0 8]\n"
     ]
    }
   ],
   "source": [
    "partitions = np.array([.8,.1,.1])\n",
    "\n",
    "print('Partition proportions:')\n",
    "print(partitions)\n",
    "print(' ')\n",
    "\n",
    "# convertimos las proporciones a números, multiplicando cada proporción por la cantidad de etiquetas en los datos\n",
    "partition_bounds = np.cumsum(partitions*len(fake_labels)).astype(int)\n",
    "#aplicamos la suma acumulada para sumar los previos números al actual\n",
    "#al multiplicar cada ratio por el tamaño de los datos, obtenemos la cantidad (índice) superior que posee esa proporción\n",
    "print('Partition boundaries:')\n",
    "print(partition_bounds)\n",
    "print(' ')\n",
    "\n",
    "\n",
    "# generamos índices aleatorios, que tengan el rango de variación del tamaño total de los datos\n",
    "rand_indxs = np.random.permutation(range(len(fake_labels)))\n",
    "print('Randomized data indices:')\n",
    "print(rand_indxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b6e4fa2-0f27-4deb-8e9b-402d8c2a82df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#de modo que, partition_bounds indica el índice límite de cada división\n",
    "#siendo las proporciones de tamaño, con la suma acumulada aplicada\n",
    "\n",
    "X_train   = fake_data[rand_indxs[:partition_bounds[0]],:]\n",
    "y_train = fake_labels[rand_indxs[:partition_bounds[0]]]\n",
    "\n",
    "# select rows for the devset data\n",
    "X_devset   = fake_data[rand_indxs[partition_bounds[0]:partition_bounds[1]],:]\n",
    "y_devset = fake_labels[rand_indxs[partition_bounds[0]:partition_bounds[1]]]\n",
    "\n",
    "# select rows for the test data\n",
    "X_test   = fake_data[rand_indxs[partition_bounds[1]:],:]\n",
    "y_test = fake_labels[rand_indxs[partition_bounds[1]:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7267fdbb-7403-4971-8097-039a11f5078d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[101, 102, 103, 104],\n",
       "       [ 21,  22,  23,  24],\n",
       "       [ 51,  52,  53,  54],\n",
       "       [ 41,  42,  43,  44],\n",
       "       [ 31,  32,  33,  34],\n",
       "       [ 61,  62,  63,  64],\n",
       "       [ 71,  72,  73,  74],\n",
       "       [ 81,  82,  83,  84]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1bf83418-ccd3-411b-a33f-d65f940015f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: (8, 4)\n",
      "Devset data size: (1, 4)\n",
      "Test data size: (1, 4)\n",
      " \n",
      "Training data: \n",
      "[[101 102 103 104]\n",
      " [ 21  22  23  24]\n",
      " [ 51  52  53  54]\n",
      " [ 41  42  43  44]\n",
      " [ 31  32  33  34]\n",
      " [ 61  62  63  64]\n",
      " [ 71  72  73  74]\n",
      " [ 81  82  83  84]]\n",
      " \n",
      "Devset data: \n",
      "[[11 12 13 14]]\n",
      " \n",
      "Test data: \n",
      "[[91 92 93 94]]\n"
     ]
    }
   ],
   "source": [
    "print('Training data size: ' + str(X_train.shape))\n",
    "print('Devset data size: '   + str(X_devset.shape))\n",
    "print('Test data size: '     + str(X_test.shape))\n",
    "print(' ')\n",
    "\n",
    "# print out the train/test data\n",
    "print('Training data: ')\n",
    "print(X_train)\n",
    "print(' ')\n",
    "\n",
    "print('Devset data: ')\n",
    "print(X_devset)\n",
    "print(' ')\n",
    "\n",
    "print('Test data: ')\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e37c72-6fdd-4a34-a2f6-91f466d79900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da5446c-c5f0-4169-98b2-56334f3b09a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dfdac2-a91d-4373-b7fe-fe9294e11800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb8346f-5614-4f6c-8daf-43d625e8a8be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17b1636-5a1f-4498-aa4b-8f73cdf0a58c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5c6ca1-6222-4f59-93cd-69404c1f370d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_workflow]",
   "language": "python",
   "name": "conda-env-pytorch_workflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
